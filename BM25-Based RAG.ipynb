{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        loader = TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF or TXT.\")\n",
    "    return loader.load()\n",
    "file_path = \"genai-principles.pdf\" \n",
    "documents = load_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Karan Singh, Assistant Professor of Operations Research \\nPrinciples of Generative AI \\nA Technical Introduction \\nGenerative artificial intelligence (GenAI) tools are an emerging class of new-age artificial \\nintelligence algorithms capable of producing novel content — in varied formats such as text, \\naudio, video, pictures, and code — based on user prompts. Recent advances in machine \\nlearning (ML), massive datasets, and substantial increases in computing power have propelled \\nsuch tools to human-level performance on academic and professional benchmarks , 1\\ncomparable to the ninetieth percentile on the SAT and the bar exam. \\nThis rapid progress has led many  to to believe that the metamorphosis of these technologies 2\\nfrom research-grade demos to accessible and easy-to-use production-grade goods and \\nservices carries the potential to supercharge business processes and operations while enabling \\nentirely new deliverables heretofore rendered infeasible by economic or technological factors. It \\ntook OpenAI’s ChatGPT, a conversational web app based on a generative (multimodal) \\nlanguage model, about five days to reach one million users  (compared to 2.5 months for 3\\nInstagram). On the business side, the Economist reports that the number of jobs mentioning AI-\\nrelated skills quadrupled from 2022 to 2023. This enthusiasm has not gone unmet by investors. \\nGenerative AI startups reportedly raised 600% more capital in 2022 than in 2020 .   4\\n \\n 1\\nFigure 1: A taxonomy of GenAI-related disciplines.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='Karan Singh, Assistant Professor of Operations Research \\nPurpose and Scope  \\nWhat are these new-era AI technologies? How do they function? What principles do they \\noperate on? What makes them different than already-hyped-up conventional machine learning \\n(ML) models? For what tasks is this class of technology most impactful? What future advances \\nmight one look forward to? These are the questions this report attempts to shed some light on. \\nThe report will also tease out how this understanding foundationally informs the best uses (and \\nmisuses) of GenAI in applied contexts. \\nA word of disclaimer: this gradient of topics also means that, while the initial sections deal with \\nfactual, if somewhat simplified, nuts-and-bolt workings of such models, the later sections delve \\ninto hopefully reasonable, but in a manner that only time may attest to, extrapolations and \\nspeculations, as necessitated by the developing nature of this technology and its current phase \\nin the technology adoption cycle. \\nWhile generative AI models come in many different shapes, utilizing varied statistical and \\ncomputational techniques to target various modalities, ranging from code and text to audio and \\nvideo, this report focuses almost exclusively on large language models (LLMs) capable of \\ngenerating novel text from textual prompts. This choice is partly due to the substantial lead \\nLLMs have in driving the overall usage of generative AI models  and partly due to the centrality 5\\nof language in formulating and addressing commonplace information-processing tasks. That \\nsaid, image- and code-based GenAI models have already witnessed successful commercial \\nproduct deployment, for example, by Adobe for creating visual content and by Github as a \\nprogramming assistance tool.   \\n 2\\nFigure 2: An image-\\nbased GenAI model, \\nMidjourney’s response to \\nthe prompt — \\n“Businessman in Tokyo \\namidst rush hour, his \\ngaze fixed ahead, \\nsurrounded by a sea of \\nblack umbrellas.”\\nFigure 3: Based on a code-based GenAI model, OpenAI Codex, \\nGithub Copilot is a commercial tool that can generate functional \\ncode from specifications given as natural language. Reportedly, as \\nof June 2023, it served over a million users.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='Karan Singh, Assistant Professor of Operations Research \\nA Quick First Introduction to Language Models \\nAt its core, a language model implements a simple functionality— to predict the next word (or \\ntoken) given a context window specifying preceding words. More precisely, given a context \\nwindow, a language model outputs a probability distribution over all possible words in its \\nvocabulary, indicating the probability with which each possible word follows the given list of \\nwords. Upon sampling  a guess of the next word from the said distribution, the language model 6\\nincrementally repeats this ostensibly primitive step to produce a more extensive body of text.    \\n \\nWe make two observations here: \\n1. Completions are random. The predicted completion, given a context window, is not \\ndeterministic. Sampling the next word in each step from the output distribution introduces \\nenough randomness to permit that the predicted completions could be meaningfully \\ndifferent on every fresh run. This stochasticity is why ChatGPT, for instance, can offer \\nvaried answers for the same prompt across successive runs. Replacing the sampling step \\nwith choosing (greedily) the most likely immediate word is known to degrade the quality of \\nthe produced text. The randomness in responses is also desirable from a user \\n 3\\nFigure 4: A probabilistic model predicting the next word coupled with sampling can produce \\nlarger bodies of text.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='Karan Singh, Assistant Professor of Operations Research \\nperspective in getting varied responses. From the deployer’s perspective, this optionally \\nallows the model to gather user feedback regarding the quality of seemingly plausible \\nresponses. This choice partly also contributes to hallucination in language models. \\n2. Initial prompt matters. Language models are conditional probabilistic models. They \\nproduce a completion conditioned on the initial set of words. In this way, the initial context \\nwindow, termed prompt, matters crucially to the produced completion. One hallmark of \\nmodern language models is that they keep track of the initial prompt even when \\ngenerating large bodies of text, unlike the earlier generation of models, thus producing \\nmore coherent responses. Artful and cleverly crafted prompts can significantly improve \\nthe quality and utility of the synthesized text. Prompt engineering , for example, practices 7\\nthat encourage the language model to solve a problem by decomposing it into \\nintermediate subproblems, has been known to improve the performance on logical \\nreasoning tasks. \\nContextualizing LLMs in terms of Recent AI Advances \\nAlthough we describe the text generation procedure above, many questions still need to be \\naddressed: How do language models function internally? How are the output probabilities for \\nthe next word determined? What goes into creating (and indeed using) a language model? How \\nare language models different from more traditional predictive models if all they do is predict the \\nnext token? \\nWe address these questions indirectly in the present section by taking a tour of the essential \\nsignificant developments in machine learning and artificial intelligence that have occurred in the \\nlast decade and have fueled the creation of modern large language models. \\nClassical Machine Learning as Prediction Machines \\nWe start with the most well-understood subset of machine learning techniques: supervised \\nlearning. The central objective in supervised learning is to produce a prediction rule that predicts \\nwell on unseen data, given enough labeled examples. For example, consider predicting house \\nprices from the square footage in a given zip code. Instead of creating a hand-crafted prediction \\nrule, the machine learning methodology advocates for choosing a prediction rule from an \\nexpressive but non-exhaustive class of rules, such as linear predictors, that provides the best fit \\non an existing collection of size-price examples. The statistically well-substantiated leap of faith \\nhere is that we expect (or at least hope) that a parsimonious prediction rule that predicts well on \\ncollected data, for which we know the correct answers, continues to maintain its predictive edge \\non unseen data, where answers or prices are unknown. Such a predictive methodology benefits \\nfrom an abundance of labeled examples, hoping that a prediction rule learned from more \\nexamples is more robust in that its superior predictive performance on seen data is less \\nascribable to chance alone. Another example of a supervised learning task is to separate spam \\nfrom non-spam mail, given the text in email messages. Again, having more examples of spam \\nand non-spam emails is helpful to a supervised learning algorithm. \\n 4'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='Karan Singh, Assistant Professor of Operations Research \\n \\nCharacteristics common to both language models and supervised learning: \\n1. Predicting Well is the Yardstick. A prediction rule is good as long as it makes \\nreasonable predictions on average. Compared to more ambitious sub-disciplines in \\nstatistics, any statements about causality, p-values, and recovering latent structure are \\nabsent. We are similarly impervious to such considerations in language models. Such \\nsimplicity of goals enables very flexible prediction rules in machine learning. Although \\nseeming modest in its aim, the art of machine learning has long been to cast as many \\ndisparate problems as questions about prediction as possible. Predicting house prices \\nfrom square footage is a regular regression task. But, for reverse image captioning, is \\n“predicting” a (high-dimensional) image given a few words a reasonable or well-defined \\nclassification task? Yet, this is how machine learning algorithms function. \\n2. Model Agnosticism. Supervised learning algorithms realize the adage that all models \\nare wrong, but some are useful. For example, when building the price predictor above, a \\ndata scientist does not believe that the genuine relationship between prices and area is \\nlinear or well-specified. Similarly, when using neural networks to predict the next word in \\nlanguage models, we don’t believe that this is how Shakespeare must have employed a \\nneural network to compose his texts. \\nYet, there are crucial differences: \\n 5\\nFigure 5: Predicting house prices from square footage. Pictured is a linear \\nregression, an example of a supervised learning algorithm that uses extant \\ndata to learn a linear predictor.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='Karan Singh, Assistant Professor of Operations Research \\n1. Fidelity of Seen Data vs. Unseen Data. Classical supervised learning operates on the \\nassumption that seen data must be representative of unseen data in a particular sense, \\nnamely that any fixed example is equally likely to be in the seen or unseen bucket. In the \\nabsence of temporal effects, this is reasonable for house prices. More generally, \\nsupervised learning requires a well-curated dataset that is closely aligned with the \\nprediction task at hand. But, as we will see, language models are trained on vast corpora \\nof somewhat ruthlessly collected texts from the internet. Yet, completing a random partial \\nsentence from the internet is presumably not what businesses using language models \\ncare about. \\nDeep Learning as Automated Representation Learning \\nAlthough useful for panel or tabular data, pre-deep-learning-era supervised algorithms struggled \\nto predict well when presented with visual or auditory inputs. Although the promise of machine \\nlearning is predicated on the automation of learning, in practice, supervised learning algorithms \\nrequire carefully crafted representations of input data in which operations like additions and \\nmultiplications, for example, for linear regression, were semantically relevant. Decades of \\npainstaking research in signal processing and computer vision had resulted in domain-specific \\nhand-crafted representations, each useful for a specific modality (images, audio, or video). The \\npredictive performance of ML algorithms was limited by how good such representations were. \\n 6\\nFigure 6: A typical deep neural network for recognizing faces. Each \\nsuccessive layer progressively learns higher-level representations (from \\nedges to contours to faces).'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='Karan Singh, Assistant Professor of Operations Research \\nThe revolution in deep learning was to automate the process of representation learning itself. \\nDeep learning uses neural networks with multiple layers, each layer incrementally converting \\nthe data into a more manageable form, all to make better predictions. This form of automated \\nhierarchical representation learning heralded a decade of tremendous progress in image and \\nspeech recognition and machine translation, starting with the breakthrough work of Krizhevsky, \\nSutskever, and Hinton  in 2012 on the Imagenet challenge. Taking advantage of GPUs (a form 8\\nof shared-memory parallel computing) and the availability of a large public dataset, this seminal \\nwork slashed the error rate for image recognition by a substantial multiple. Parallel gains were \\nlater realized using similar deep neural network architectures in speech recognition and other \\nmachine learning domains. In this sense, the advances deep learning enabled were (relatively) \\ndomain agnostic. \\nAlthough deep neural networks are data-hungry in that they require a substantially large dataset \\nto start predicting well, they also successfully realize a long-promised advantage of neural \\nnetworks. This factor is crucial to the practice of modern-day machine learning. In the process of \\nhierarchically learning representations, deep nets learn task- (or label--) agnostic features of the \\ndataset in the lower layers, while higher layers closer to the output account for task-specific \\nrepresentations. This permits us to (a) train a deep net to separate images of cats and dogs on \\na large dataset and (b) subsequently build a shallow (even linear) performant neural net that \\nuses the lower layers of the former to craft useful representations to classify images of zebra \\nand giraffes. Step A is often called pre-training, and step B is referred to as supervised fine-\\ntuning. This manner of amortizing the learning across tasks that are not individually data-rich is \\ncentral to language models. \\nWord Embeddings and Contrastive Learning \\nWhile the progress of deep learning in speech and audio was made possible by the availability \\nof large crowd-labeled datasets (with 10s of millions of annotated images), such large high-\\nquality datasets were absent in the textual domain, despite a plethora of unlabelled data in the \\nform of books, Wikipedia articles, and articles on the internet. Could a machine learning \\nalgorithm make use of the cheap, unlabelled data instead? \\nIn computational linguistics, the distributional hypothesis codifies an appealing and intuitive idea \\nthat similar words occur in similar contexts. In 2013, inspired by this observation, Mikolov et al  9\\ntrained a neural network, termed Word2Vec, to predict randomly selected words in a text corpus \\ngiven neighboring words for each. Note that this step doesn’t require any need human \\nannotators. They observed that the 300-dimensional vector representations the neural net \\nlearned for words had excellent linear algebraic properties that transparently reflected the \\nunderlying semantics. For example, one obtained Queen when queried for the word with the \\nvector closest to King - Man + Woman. Thus, each vector dimension captured some abstract \\nsemantic degree of freedom. These representations were also valuable for natural classification \\ntasks with limited data, such as sentiment classification, given a small number of examples. \\n 7'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='Karan Singh, Assistant Professor of Operations Research \\n \\nThe approach of creating auxiliary labeling tasks for free from unlabelled data to learn \\nsemantically relevant representation is called contrastive learning and has proved helpful in \\nother domains, too. For example, given a set of unlabelled images, a classifier trained to \\nrecognize random crops from the same image as a positive match and those from distinct \\nimages as a negative match (pre-training step) learns representations useful for supervised fine-\\ntuning on genuine classification tasks downstream. \\nTransformers mollify the Optimization Landscape \\nWhile word embeddings serve as proof that textual semantic regularities can be assessed \\nwithout labeled data, substantive language processing tasks need an algorithmic \\nimplementation of the concept of memory to capture relationships between words that are \\npositionally far apart. For example, a common motif in stories is that the next act derives from \\nsome event that occurred a while ago.  \\n 8\\nFigure 7: Vector space representations of words exhibit linear algebraic \\nrelationships between semantic units and can be used to answer analogy \\nquestions, e.g., son - father + mother = daughter.\\nFigure 8: RNNs capture memory effects by sequentially processing \\ninformation.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Karan Singh, Assistant Professor of Operations Research \\nThe first generation of neural networks that captured the notion of memory were Recurrent \\nNeural Networks (RNNs), by sequentially processing a piece of text one word at a time while \\nupdating an internal state to maintain continuity, a proxy for memory. Unfortunately, optimizing \\nsuch recurrent neural nets to find one that best fits a given dataset proved extra-ordinarily error-\\nprone and challenging. \\nIn 2017, Vaswani et al  introduced a different neural network architecture, termed transformer, 10\\nthat could efficiently capture long-range relations between tokens compactly (non-sequentially) \\nby processing the entire surrounding context window at once while remaining amenable to \\ngradient-based optimization. The introduction of transformers spurred a line of research on \\nlanguage models, culminating in training models with an increasingly higher number of \\nparameters trained on ever larger datasets. For example, GPT2 (Generative Pre-trained \\nTransformer 2), released in 2019, is a 1.5 billion parameter model trained on 40 GB of data, \\nwhile GPT3, released in 2020, is a 175 billion parameter model trained on 570 GB of text data. \\nWhile larger models resulted in better performance, the open-market cost for training these \\nenormous models was estimated to be tens of millions of dollars. \\n \\nGeneral-Purpose Language Models: Supervised Fine-tuning & GPT3 \\nThe general paradigm brought about by contrastive learning was first to learn a large model on \\nauxiliary tasks created using an unlabelled dataset (the pre-training step) and subsequently to \\nuse these learned representations in a downstream supervised learning task given a few task-\\nspecific labeled examples (the supervised fine-tuning step). While broadly useful and practical, \\nsupervised fine-tuning requires replicas of the baseline pre-trained model for each downstream \\n 9\\nFigure 9: The LLM arms race with exponentially increasing \\nparameter counts. (Credit: HuggingFace)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='Karan Singh, Assistant Professor of Operations Research \\ntask; further, the large size of language models makes running even a few steps of gradient-\\nbased iterative optimization for supervised learning prohibitive except on computationally \\nexpensive hardware setups. \\nThe paper  describing the architecture of the GPT3 model presents a far cheaper and more 11\\nconvenient way of repurposing pre-trained language models for specific downstream tasks, \\nnamely, by specifying a few labeled examples in the prompt before asking for a label or \\nresponse for unseen data. This mode of inference, in-context learning, does not require \\ncomputationally expensive adjustments to the weights or parameters of an LLM and instead \\ntreats the entire downstream supervised task as a prompt for the language model to complete. \\nThis makes LLMs very attractive for end-users, who no longer have to create copies of the large \\nmodel to customize, nor do they have to run a sophisticated optimization procedure to adjust \\nparameters; each downstream task, in effect, becomes a conversation. While fine-tuning may \\nstill result in additional performance gains over in-context learning for some tasks in exchange \\nfor a massive increase in computational load, a crucial advance of GPT3 is that this \\nsubstantially lowers this gap, democratizing the use (although not the training) of LLMs. \\n \\n 10\\nFigure 10: An illustration of in-context learning. GPT4 figures out the \\ncorrect pattern that the answer is the first number + reverse of the \\nsecond, given two examples.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Karan Singh, Assistant Professor of Operations Research \\nTowards Conversational AI: Learning from Human Feedback \\nWhile GPT3-like models happen to be good at conversation-centered tasks, they are not \\nexplicitly trained or incentivized to follow instructions. OpenAI’s InstructGPT model  post pre-12\\ntraining aligns the model to follow the users’ instructions by fine-tuning the model to mimic \\nlabeled demonstrations of the desired behavior (via supervised learning) and highly-ranked \\nresponses to prompts as collected using human feedback (via reinforcement learning). \\n \\nThe Future: Foundation Models \\nGiven the success of language models, there has been increased interest in the possibility of \\nrecreating the magic of LLMs in other domains. Such models, generically termed foundation \\nmodels, attempt to amortize the cost of limited-data downstream tasks by pre-training on large \\ncorpora of broadly related tasks or unlabelled datasets. For example, one might be able to \\nrepurpose the LLM paradigm to train a generalist robot or decision-making agent that learns \\nfrom supply chain operations across all industries. \\nConclusion \\nThis report contextualizes large-language models within the more extensive machine learning \\nand artificial intelligence landscape by training the origins of the principal ideas that fuel today’s \\nlarge language models. By bringing out their essential characteristics and differences against \\ntraditional modes of machine learning, we hope that a user of such models can be better \\n 11\\nFigure 11: While GPT3 performs text completion by guessing the \\nmost plausible completion, InstructGPT has been explicitly \\ntrained to follow instructions. (Credit: OpenAI’s web report)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.0 (Build 23A344) Quartz PDFContext', 'creator': 'Pages', 'creationdate': '2023-10-25T02:42:03+00:00', 'author': 'Karan Singh', 'moddate': '2023-10-25T09:40:35-04:00', 'title': 'GenAI-Principles', 'source': 'genai-principles.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Karan Singh, Assistant Professor of Operations Research \\ninformed of the underlying tradeoffs such models induce, e.g., the performance-resource \\ntradeoffs between fine-tuning and in-context learning. \\nEndnotes \\n See the ﬁrst table on OpenAI’s announcement for an overview of GPT4’s performance on other academic, 1\\nprofessional and programming exams. The quoted nineMeth percenMle performance on the bar exam was assessed \\nby Katz et al, but others have raised concerns. \\n See quotes by industry and research leaders here.2\\n See iniMal consumer adopMon staMsMcs for ChatGPT here and here.3\\n See this reporMng for investments in GenAI.4\\n See current and project user bases for GenAI here.5\\n When producing text, rather than sampling the next word incrementally, a more systemaMc search operaMon 6\\ntermed Beam Search, coined by Raj Reddy at CMU, oXen yields beYer results.\\n Structuring iniMal text to elicit useful outputs from GenAI model is called prompt engineering.7\\n See the full Krizhevshy, Sutskever, Hinton paper here.8\\n See the Word2Vec paper here.9\\n See the paper that introduced Transformers here.10\\n See the GPT3 paper here.11\\n See the instruct GPT paper here.12\\n 12')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(documents, chunk_size=500, chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_into_chunks(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bm25_index(chunks):\n",
    "    corpus = [chunk.page_content for chunk in chunks]\n",
    "    tokenized_corpus = [doc.split() for doc in corpus]\n",
    "    return BM25Okapi(tokenized_corpus), corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25, corpus = create_bm25_index(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(query, bm25, corpus, top_n=3):\n",
    "    tokenized_query = query.split()\n",
    "    top_docs = bm25.get_top_n(tokenized_query, corpus, n=top_n)\n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents\n",
      " ['Karan Singh, Assistant Professor of Operations Research \\nPurpose and Scope  \\nWhat are these new-era AI technologies? How do they function? What principles do they \\noperate on? What makes them different than already-hyped-up conventional machine learning \\n(ML) models? For what tasks is this class of technology most impactful? What future advances \\nmight one look forward to? These are the questions this report attempts to shed some light on.', 'Karan Singh, Assistant Professor of Operations Research \\nPrinciples of Generative AI \\nA Technical Introduction \\nGenerative artificial intelligence (GenAI) tools are an emerging class of new-age artificial \\nintelligence algorithms capable of producing novel content — in varied formats such as text, \\naudio, video, pictures, and code — based on user prompts. Recent advances in machine \\nlearning (ML), massive datasets, and substantial increases in computing power have propelled', 'language model, about five days to reach one million users  (compared to 2.5 months for 3\\nInstagram). On the business side, the Economist reports that the number of jobs mentioning AI-\\nrelated skills quadrupled from 2022 to 2023. This enthusiasm has not gone unmet by investors. \\nGenerative AI startups reportedly raised 600% more capital in 2022 than in 2020 .   4\\n \\n 1\\nFigure 1: A taxonomy of GenAI-related disciplines.']\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Generative Ai?\"\n",
    "retrieved_docs = bm25_search(query, bm25, corpus)\n",
    "print(\"Retrieved Documents\\n\", retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_1724\\741337463.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response\n",
      "  Generative Artificial Intelligence (GenAI) is a new class of artificial intelligence algorithms that can create novel content based on user prompts. This content can take various forms, such as text, audio, video, images, and code. The functioning of GenAI relies heavily on recent advancements in machine learning (ML), vast datasets, and increased computing power.\n",
      "\n",
      "Compared to conventional machine learning models, GenAI is more versatile and dynamic since it generates new content instead of merely recognizing patterns or making predictions based on existing data. Furthermore, these models operate by using statistical patterns within the given dataset to generate new content that resembles but is not exactly the same as the original data points.\n",
      "\n",
      "GenAI technology is particularly impactful in tasks such as creating personalized content for users, generating high-quality synthetic images or videos for various purposes like gaming, entertainment, and training, writing articles or stories, and even composing music. The future of GenAI looks promising with potential advancements in real-time content generation, improved quality and versatility of generated content, and increased applicability across various industries.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "def generate_response(query, retrieved_docs):\n",
    "    llm = Ollama(model=\"mistral\")\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    response = llm.invoke(f\"Using this information:\\n{context}\\nAnswer: {query}\")\n",
    "    return response\n",
    "\n",
    "response = generate_response(query, retrieved_docs)\n",
    "print(\"Response\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
