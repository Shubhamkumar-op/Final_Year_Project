{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 pages from genai-principles.pdf\n"
     ]
    }
   ],
   "source": [
    "file_path = \"genai-principles.pdf\"\n",
    "documents = load_document(file_path)\n",
    "print(f\"Loaded {len(documents)} pages from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_into_chunks(docs, chunk_size=500, chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 63\n"
     ]
    }
   ],
   "source": [
    "chunks = split_into_chunks(documents)\n",
    "print(f\"Total Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Index Created!\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = [chunk.page_content for chunk in chunks]\n",
    "tokenized_corpus = [text.split() for text in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "print(\"BM25 Index Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Index Created & Stored Locally\n"
     ]
    }
   ],
   "source": [
    "embedding_model = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "faiss_index = FAISS.from_documents(chunks, embedding_model)\n",
    "faiss_index.save_local(\"faiss_store\")\n",
    "print(\"FAISS Index Created & Stored Locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss():\n",
    "    return FAISS.load_local(\"faiss_store\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Index Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "faiss_index = load_faiss()\n",
    "print(\"FAISS Index Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, bm25_model, faiss_index, corpus, top_n=3):\n",
    "    bm25_results = bm25.get_top_n(query.split(), corpus, n=top_n)\n",
    "    faiss_results = faiss_index.similarity_search(query, k=top_n)\n",
    "    retrieved_docs = bm25_results + [doc.page_content for doc in faiss_results]\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents\n",
      " ['While generative AI models come in many different shapes, utilizing varied statistical and \\ncomputational techniques to target various modalities, ranging from code and text to audio and \\nvideo, this report focuses almost exclusively on large language models (LLMs) capable of \\ngenerating novel text from textual prompts. This choice is partly due to the substantial lead \\nLLMs have in driving the overall usage of generative AI models  and partly due to the centrality 5', 'supervised learning requires a well-curated dataset that is closely aligned with the \\nprediction task at hand. But, as we will see, language models are trained on vast corpora \\nof somewhat ruthlessly collected texts from the internet. Yet, completing a random partial \\nsentence from the internet is presumably not what businesses using language models \\ncare about. \\nDeep Learning as Automated Representation Learning', 'Karan Singh, Assistant Professor of Operations Research \\nPurpose and Scope  \\nWhat are these new-era AI technologies? How do they function? What principles do they \\noperate on? What makes them different than already-hyped-up conventional machine learning \\n(ML) models? For what tasks is this class of technology most impactful? What future advances \\nmight one look forward to? These are the questions this report attempts to shed some light on.', 'Karan Singh, Assistant Professor of Operations Research \\nPrinciples of Generative AI \\nA Technical Introduction \\nGenerative artificial intelligence (GenAI) tools are an emerging class of new-age artificial \\nintelligence algorithms capable of producing novel content — in varied formats such as text, \\naudio, video, pictures, and code — based on user prompts. Recent advances in machine \\nlearning (ML), massive datasets, and substantial increases in computing power have propelled', 'While generative AI models come in many different shapes, utilizing varied statistical and \\ncomputational techniques to target various modalities, ranging from code and text to audio and \\nvideo, this report focuses almost exclusively on large language models (LLMs) capable of \\ngenerating novel text from textual prompts. This choice is partly due to the substantial lead \\nLLMs have in driving the overall usage of generative AI models  and partly due to the centrality 5', 'factual, if somewhat simplified, nuts-and-bolt workings of such models, the later sections delve \\ninto hopefully reasonable, but in a manner that only time may attest to, extrapolations and \\nspeculations, as necessitated by the developing nature of this technology and its current phase \\nin the technology adoption cycle. \\nWhile generative AI models come in many different shapes, utilizing varied statistical and']\n"
     ]
    }
   ],
   "source": [
    "query = \"what is generative ai.\"\n",
    "retrieved_docs = hybrid_search(query, bm25, faiss_index, corpus)\n",
    "print(\"Retrieved Documents\\n\", retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_25596\\471916608.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answern  Generative Artificial Intelligence (GenAI) refers to a class of emerging artificial intelligence algorithms that can produce novel content based on user prompts. This content can be in various formats such as text, audio, video, pictures, or code. In the context of this report, the focus is mainly on Large Language Models (LLMs) capable of generating novel text from textual prompts. These models utilize substantial datasets and powerful computational techniques to achieve their tasks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "def generate_response(query, retrieved_docs):\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    return llm.invoke(f\"Context:\\n{context}\\nQuestion: {query}\\nAnswer:\")\n",
    "response = generate_response(query, retrieved_docs)\n",
    "print(\"Answern\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
